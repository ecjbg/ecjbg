{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgww2QxyS1qX4yTbsXpY6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecjbg/ecjbg/blob/main/ChatbotGroq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoO1BYCGmyxQ",
        "outputId": "8bb6ca86-b3d1-41c7-897c-b991e1fbb9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "secret_token = userdata.get('hff')\n",
        "\n",
        "if secret_token is not None:\n",
        "  print(\"successfully retrieved the secret token.\")\n",
        "\n",
        "else:\n",
        "  print(\"secret token not found.\")\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = secret_token\n",
        "\n",
        "client = InferenceClient(\"mistralai/Mistnal-7B-Instruct-v0.2\")\n",
        "client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfGEZvAjwNMg",
        "outputId": "68087685-428a-4eba-e65c-cd85af3e0ab8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully retrieved the secret token.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<InferenceClient(model='mistralai/Mistnal-7B-Instruct-v0.2', timeout=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "token = userdata.get('hff')\n",
        "\n",
        "client = InferenceClient(token=token)\n",
        "print(\"--- Running Chat Completion ---\")\n",
        "response = client.chat_completion(\n",
        "    messages=[{\"role\":\"user\", \"content\":\"Once upon a million years\"}],\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    max_tokens=50,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp1BOgsNyOa5",
        "outputId": "537ab1d2-b01a-4f4e-e6de-4ad3213d11bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Chat Completion ---\n",
            " Once upon a million years, the earth was a vastly different place. The continents were in different positions, the oceans covered more land than they do now, and creatures that roamed the earth were vastly different from those that exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio groq -q\n",
        "\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "groq_api_key = userdata.get('npy_key')\n",
        "\n",
        "if not groq_api_key:\n",
        "  print(\"please add your groq_api_key to colab secrets\")\n",
        "else :\n",
        "  print(\"Groq API key loaded.\")\n",
        "TASK_CONFIG = {\n",
        "    \"General Chat(Llama3)\" : {\n",
        "        \"model\" : \"llama3-8b-8192\",\n",
        "        \"system_prompt\" : '''you are a helpful, respectful, and honest assistant.\n",
        "        keep your responses concise and to the point''',\n",
        "    },\n",
        "}\n",
        "\n",
        "def task_select_dd_changed(task_selection):\n",
        "  \"\"\"Update the system prompt when the task selection changes.\"\"\"\n",
        "  config=TASK_CONFIG[task_selection]\n",
        "  return gr.update(value=config[\"system_prompt\"])"
      ],
      "metadata": {
        "id": "N47ABoN43L9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 1: Install\n",
        "!pip install gradio groq -q\n",
        "\n",
        "# âœ… Step 2: Import and load Groq key from Colab secrets\n",
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "groq_api_key = userdata.get('npy_key')  # Set this in Colab: Runtime > Secrets\n",
        "if not groq_api_key:\n",
        "  print(\"Please add your Groq API key to Colab secrets as 'npy_key'\")\n",
        "else:\n",
        "  print(\"Groq API key loaded.\")\n",
        "\n",
        "# âœ… Step 3: Initialize Groq client\n",
        "client = Groq(api_key=groq_api_key)\n",
        "\n",
        "# âœ… Step 4: Task configuration\n",
        "TASK_CONFIG = {\n",
        "    \"General Chat (Llama3)\": {\n",
        "        \"model\": \"llama3-8b-8192\",\n",
        "        \"system_prompt\": '''You are a helpful, respectful, and honest assistant. Keep your responses concise and to the point.''',\n",
        "    }\n",
        "}\n",
        "\n",
        "# âœ… Step 5: Dropdown handler\n",
        "def task_select_dd_changed(task_selection):\n",
        "    config = TASK_CONFIG[task_selection]\n",
        "    return gr.update(value=config[\"system_prompt\"])\n",
        "\n",
        "# âœ… Step 6: Chatbot logic\n",
        "def chat_with_groq(user_input, chat_history, task_selection, system_prompt):\n",
        "    config = TASK_CONFIG[task_selection]\n",
        "    model = config[\"model\"]\n",
        "\n",
        "    # Build messages with system prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    for user, bot in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Call Groq API\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    chat_history.append((user_input, reply))\n",
        "    return chat_history, chat_history\n",
        "\n",
        "# âœ… Step 7: Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸ¤– Groq LLaMA3 Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        task_selection = gr.Dropdown(choices=list(TASK_CONFIG.keys()), value=\"General Chat (LLaMA3)\", label=\"Select Task\")\n",
        "        system_prompt = gr.Textbox(label=\"System Prompt\", lines=2, interactive=True)\n",
        "\n",
        "    task_selection.change(fn=task_select_dd_changed, inputs=task_selection, outputs=system_prompt)\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    state = gr.State([])\n",
        "    msg = gr.Textbox(label=\"Type your message here\")\n",
        "\n",
        "    msg.submit(chat_with_groq, [msg, state, task_selection, system_prompt], [chatbot, state])\n",
        "    gr.Button(\"Clear Chat\").click(lambda: ([], []), None, [chatbot, state])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "5G87xdv9-ZDv",
        "outputId": "3e004ed0-97f2-4abf-b255-d2286e22c22c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq API key loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/dropdown.py:230: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: General Chat (LLaMA3) or set allow_custom_value=True.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-398991888.py:62: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://95a6f37c1a7275254d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://95a6f37c1a7275254d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}